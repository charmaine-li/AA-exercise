{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os       as os\n",
    "import numpy    as np\n",
    "import datetime as dt\n",
    "import sys      as sys\n",
    "\n",
    "##\n",
    "## Graphical libs and setup\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    " \n",
    "#print(plt.style.available)\n",
    "plt.style.use(['ggplot', 'fast'])\n",
    "plt.rc('image', cmap='cubehelix') # See https://www.mrao.cam.ac.uk/~dag/CUBEHELIX/\n",
    " \n",
    "import seaborn  as sns\n",
    "import pandas   as pd\n",
    "pd.options.display.width = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con1 = duckdb.connect(':memory:', config={'allow_unsigned_extensions' : 'true'})\n",
    "\n",
    "# Edge node has 504G\n",
    "con1.sql(\"SET memory_limit = '100GB'\")  \n",
    "\n",
    "# See https://stackoverflow.com/questions/71952623/reading-partitioned-parquet-files-in-duckdb\n",
    "con1.sql(\"SET temp_directory = '/data/disk1/tmp/duckdbcaches/\" + os.environ.get('USER') + \"'\")\n",
    "\n",
    "# Use progress bar (if possible)\n",
    "print(con1.sql(\"PRAGMA enable_print_progress_bar\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data\n",
    "df_fin = pd.read_csv('business-financial-data-march-2024-csv.csv')\n",
    "\n",
    "#Make all column names lower case and replace spaces with underscore\n",
    "df_fin.columns = [str(col).lower().replace(' ','_') for col in df_fin.columns]\n",
    "\n",
    "#Extract year from period as int for later analysis\n",
    "df_fin['year'] = df_fin['period'].astype(str).str.slice(0,4).astype(int)\n",
    "\n",
    "#Rename group to ind_group, as group is a reserved name\n",
    "df_fin.rename(columns={'group':'ind_group'}, inplace=True)\n",
    "\n",
    "df_fin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data\n",
    "df_emp = pd.read_csv('machine-readable-business-employment-data-mar-2024-quarter.csv')\n",
    "\n",
    "#Rename all columns to lower case and replace spaces with underscore\n",
    "df_emp.columns = [str(col).lower().replace(' ','_') for col in df_emp.columns]\n",
    "\n",
    "#Rename group to ind_group because group is a reserved name\n",
    "df_emp.rename(columns={'group':'ind_group'}, inplace=True)\n",
    "\n",
    "df_emp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register data frames as views\n",
    "duckdb.register('fin',df_fin)\n",
    "duckdb.register('emp',df_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dtypes\n",
    "display(df_fin.dtypes)\n",
    "display(df_emp.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Identifying Industry with Most Filled Jobs (Post-2016 Salaries and Wages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_query = \"\"\"\n",
    "with sal_wgs as\n",
    "(--Get all salaries and wages data\n",
    "select \n",
    "    *\n",
    "from fin \n",
    "\n",
    "where series_title_1 = 'Salaries and wages'\n",
    "      \n",
    ")\n",
    "\n",
    ", first_year as\n",
    "(--All industries where first year for salaries and wages was after 2016\n",
    "select\n",
    "    series_title_2\n",
    "    , ind_group\n",
    "    , min(year) as first_year\n",
    "from sal_wgs\n",
    "\n",
    "group by series_title_2, ind_group\n",
    "\n",
    "having min(year) > 2016\n",
    ")\n",
    "\n",
    "--Get industry with highest average filled jobs\n",
    "select\n",
    "    mx.series_title_2 as industry\n",
    "    ,mx.avg_filled_jobs as overall_avg_filled_jobs\n",
    "from \n",
    "(--Average actual filled jobs per industry\n",
    "    select\n",
    "        emp.series_title_2\n",
    "        ,avg(emp.data_value) as avg_filled_jobs\n",
    "        ,emp.series_title_1\n",
    "    from emp \n",
    "    \n",
    "    inner join first_year fst on fst.series_title_2 = emp.series_title_2 --Noted that emp dataset only includes NZSIOC Level 2\n",
    "    --Actual filled jobs only\n",
    "    where emp.series_title_1 = 'Filled jobs'\n",
    "          and emp.series_title_3 = 'Actual'\n",
    "\n",
    "    group by emp.series_title_2\n",
    "            , emp.series_title_1\n",
    ") mx\n",
    "\n",
    "order by avg_filled_jobs desc\n",
    "\n",
    "limit 1\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "result = duckdb.sql(Q1_query).df()\n",
    "print('Top industry by actual filled jobs:')\n",
    "display(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Identifying NZSIOC Level 2 Industry with Second Highest Seasonally Adjusted Operating Income Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2_query = \"\"\"\n",
    "with base as \n",
    "(--Get level 2 industries and seasonally adjusted operating income data \n",
    "select \n",
    "    *\n",
    "from fin\n",
    "where series_title_1 = 'Sales (operating income)'\n",
    "      and ind_group = 'Industry by financial variable (NZSIOC Level 2)'\n",
    "      and series_title_4 = 'Seasonally adjusted'\n",
    ")\n",
    "\n",
    "--Identify second highest operating income industry and the period\n",
    "select \n",
    "    period\n",
    "    ,series_title_2 as industry\n",
    "    ,data_value as operating_income\n",
    "from\n",
    "    (--Rank seasonally adjusted operating income largest to smallest across all periods and industries\n",
    "    select\n",
    "        period\n",
    "        ,series_title_2\n",
    "        ,data_value\n",
    "        ,dense_rank() over (order by data_value desc) as operating_inc_rnk\n",
    "    from base\n",
    "    ) rnk\n",
    "where operating_inc_rnk = 2\n",
    "\"\"\"\n",
    "\n",
    "result = duckdb.sql(Q2_query).df()\n",
    "print('Second highest industry by seasonally adjusted operating income and period:')\n",
    "display(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Three: Calculating Cumulative Number of Filled Jobs for the Top Territorial Authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3_query = \"\"\"\n",
    "with \n",
    "base as \n",
    "(--Get all territorial authorities and filled jobs data\n",
    "select\n",
    "      series_title_2\n",
    "      ,data_value\n",
    "      ,period\n",
    "from emp\n",
    "where ind_group = 'Territorial authority by employment variable'\n",
    "      and series_title_1 = 'Filled jobs'\n",
    ")\n",
    "\n",
    ",highest_avg as\n",
    "(--Get the territorial authority with the highest average value for filled jobs \n",
    "select\n",
    "      series_title_2 as territorial_authority\n",
    "      ,avg_filled_jobs\n",
    "from      \n",
    "      (--Calculate average filled jobs per territorial authority\n",
    "      select\n",
    "            series_title_2\n",
    "            ,avg(data_value) as avg_filled_jobs\n",
    "      from base\n",
    "      group by series_title_2\n",
    "      ) avrg\n",
    "order by avg_filled_jobs desc\n",
    "limit 1      \n",
    ")\n",
    "\n",
    ",top_ta as\n",
    "(--Get actual filled jobs per period for territorial authority identified in highest_avg CTE\n",
    "select\n",
    "      base.period\n",
    "      ,ha.territorial_authority\n",
    "      ,base.data_value as filled_jobs\n",
    "from highest_avg as ha\n",
    "inner join base on base.series_title_2 = ha.territorial_authority\n",
    ")\n",
    "\n",
    "--Calculate quarterly cumulative number of filled jobs over time\n",
    "select      \n",
    "      ta.period\n",
    "      ,ta.territorial_authority\n",
    "      ,ta.filled_jobs\n",
    "      ,sum(prd.filled_jobs) as cumulative_filled_jobs\n",
    "from top_ta ta\n",
    "inner join top_ta prd on prd.period <= ta.period\n",
    "group by ta.period, ta.territorial_authority, ta.filled_jobs\n",
    "order by ta.period\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "result = duckdb.sql(Q3_query).df()\n",
    "print('Quarterly cumulative filled jobs:')\n",
    "display(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Four: Data Pipelines\n",
    "\n",
    "We could use libraries like Pandas to check the data for such aberrations, and that they only include values we expect. <br>\n",
    "\n",
    "For duplicates, we should first check if there are any duplicates, and drop them if so. <br>\n",
    "\n",
    "e.g. \n",
    "```python\n",
    "df_fin_dedupe = df_fin\n",
    "\n",
    "#Check for duplicates\n",
    "df_fin_dedupe[df_fin_dedupe.duplicated() == True]\n",
    "\n",
    "#Drop duplicates\n",
    "df_fin_dedupe.drop_duplicates()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify incorrect data types, we would first want to see what data types the datasets contain, and if they are what we expected, using df.dtypes.\n",
    "Since the datasets are part of a pipeline, we should know what columns and data types we expect to see for each. <br>\n",
    "Thus, we can first define a dictionary showing the columns and their expected data types.\n",
    "For example, using the financial data csv, we may define something like the below, and then check the actual data types of our dataset against this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_dedupe = df_fin.copy()\n",
    "\n",
    "#Create a dictionary of expected data types per column\n",
    "expected_schema = {\n",
    "    'series_reference': 'object',\n",
    "    'period': 'object',\n",
    "     'data_value': 'float64',\n",
    "     'suppressed': 'object',\n",
    "     'status': 'object',\n",
    "     'units': 'object',\n",
    "     'magnitude': 'int64',\n",
    "     'subject': 'object',\n",
    "     'ind_group': 'object',\n",
    "     'series_title_1': 'object',\n",
    "     'series_title_2': 'object',\n",
    "     'series_title_3': 'object',\n",
    "     'series_title_4': 'object',\n",
    "     'series_title_5': 'object',\n",
    "     'year': 'int64'\n",
    "}\n",
    "\n",
    "#Then, we would want to check the actual data types of our dataset against this dictionary\n",
    "for col, dtype in expected_schema.items():\n",
    "    assert df_fin_dedupe[col].dtype == dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this, we will see an AssertionError because period and series_title_5 are not our expected data types of object. <br>\n",
    "To improve the pipeline, a function could then be written to compare the expected data types against the actual, and if they are different, then cast the actual to the expected. <br>\n",
    "e.g. The function could be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_schema(df_fin_dedupe, expected_schema):\n",
    "    for col, expected_dtype in expected_schema.items():\n",
    "        actual_dtype = df_fin_dedupe[col].dtype\n",
    "\n",
    "        # Skip if it's already correct\n",
    "        if str(actual_dtype) == expected_dtype:\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Column '{col}' is {actual_dtype}, expected {expected_dtype}. Attempting to cast...\")\n",
    "\n",
    "        try:\n",
    "            if expected_dtype == 'float64':\n",
    "                df_fin_dedupe[col] = pd.to_numeric(df_fin_dedupe[col], errors='coerce')\n",
    "            elif expected_dtype == 'int64':\n",
    "                df_fin_dedupe[col] = pd.to_numeric(df_fin_dedupe[col], errors='coerce').astype('Int64')\n",
    "            elif  expected_dtype == 'object':\n",
    "                df_fin_dedupe[col] = df_fin_dedupe[col].fillna('').astype(str)\n",
    "            else:\n",
    "                print(f\"[WARN] Unhandled type: {expected_dtype}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to cast column '{col}' to {expected_dtype}: {e}\")\n",
    "\n",
    "    return df_fin_dedupe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_stg = enforce_schema(df_fin_dedupe,expected_schema)\n",
    "print(df_fin_stg.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for missing dates, we may want to define the range of dates we are expecting for each dataset.\n",
    "From there, we can check if the dataset is missing any of these dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the date range we are expecting\n",
    "expected_periods = pd.date_range('2016-01-01','2024-03-31', freq = 'Q')\n",
    "\n",
    "#Define missing dates as ones not within the expected_periods\n",
    "missing_periods = set(expected_periods) - set(pd.to_datetime(df_fin_stg['period'], format = '%Y.%m')+ pd.offsets.MonthEnd(0))\n",
    "\n",
    "display(missing_periods)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For aberrations such as other missing values, we may want to drop any key rows with missing data, or fill with another value, such 'Unknown'. This would help prevent misinterpretation of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop any rows with NULL in data_value column\n",
    "df_fin_na = df_fin_stg.dropna(subset = ['data_value'])\n",
    "\n",
    "#Fill any NAs or missing values for suppressed and series_title_5 with 'Unknown'\n",
    "df_fin_fill = df_fin_stg.replace('',pd.NA).fillna({\n",
    "    'suppressed': 'Unknown',\n",
    "    'series_title_5': 'Unknown'\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may also be worth considering why certain values are missing to assist with analysis further down the line.\n",
    "For example, are they missing completely at random, missing at random or missing not at random? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Five: Correlation Between Actual Filled Jobs and Unadjusted Sales by Industry\n",
    "We will explore if there is a correlation between actual filled jobs and unadjusted sales (operating income) for the NZSIOC Level 2 industries per period. <br>\n",
    "We should expect to see a positive correlation between the two, as generally more labour will result in higher productivity, and therefore higher sales. <br>\n",
    "However, we should expect this to vary significantly between industries, as well as see clearly defined cut-offs between industries in number of filled jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter employment data for relevant observations\n",
    "df_emp_jbs = df_emp[\n",
    "    (df_emp['ind_group']=='Industry by employment variable') &\n",
    "    (df_emp['series_title_1']=='Filled jobs') &\n",
    "    (df_emp['series_title_3']=='Actual')\n",
    "    ]\n",
    "\n",
    "# Filter financial data for relevant observations\n",
    "df_fin_inc = df_fin[\n",
    "    (df_fin['ind_group']== 'Industry by financial variable (NZSIOC Level 2)') &\n",
    "    (df_fin['series_title_1']=='Sales (operating income)') &\n",
    "    (df_fin['series_title_4']=='Unadjusted')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_fin_inc.head())\n",
    "display(df_emp_jbs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by to ensure correct level of aggregation\n",
    "emp_summary = df_emp_jbs.groupby(['period','series_title_2']).agg({'data_value':'mean'}).reset_index()\n",
    "\n",
    "#Rename columns\n",
    "emp_summary.rename(columns={'data_value':'filled_jobs', 'series_title_2': 'industry'}, inplace = True)\n",
    "\n",
    "display(emp_summary.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by to ensure correct level of aggregation\n",
    "inc_summary = df_fin_inc.groupby(['period','series_title_2']).agg({'data_value':'mean'}).reset_index()\n",
    "\n",
    "#Rename columns\n",
    "inc_summary.rename(columns={'data_value':'sales', 'series_title_2': 'industry'}, inplace = True)\n",
    "\n",
    "display(inc_summary.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge above tables together\n",
    "df_inc_jbs = pd.merge(emp_summary, inc_summary, on =['period','industry'])\n",
    "\n",
    "#Remove any NA values so it doesn't affect analysis later on\n",
    "df_inc_jbs.dropna(inplace = True)\n",
    "\n",
    "df_inc_jbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print('Summary statistics:')\n",
    "display(df_inc_jbs.describe())\n",
    "\n",
    "print('Observation count per industry:')\n",
    "display(df_inc_jbs['industry'].value_counts())\n",
    "\n",
    "print('Average sales (operating income) per industry:')\n",
    "display(df_inc_jbs['sales'].groupby(df_inc_jbs['industry']).mean())\n",
    "\n",
    "print('Average filled jobs per industry:')\n",
    "display(df_inc_jbs['filled_jobs'].groupby(df_inc_jbs['industry']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(data=df_inc_jbs, x='filled_jobs', y='sales', hue ='industry', style ='industry', palette = 'colorblind')\n",
    "plt.title('Filled Jobs vs Sales (Operating Income) by Industry')\n",
    "plt.xlabel('Filled Jobs')\n",
    "plt.ylabel('Sales (Operating Income)')\n",
    "plt.legend(bbox_to_anchor=(1.05,1), loc = 'upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of filled jobs to sales by industry\n",
    "ind_corrs = (\n",
    "    df_inc_jbs.groupby('industry')[['filled_jobs','sales']].corr().unstack().iloc[:,1].reset_index(name ='correlation')\n",
    ")\n",
    "print(ind_corrs.sort_values(by='correlation',ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance of filled jobs and sales by industry\n",
    "ind_std = (\n",
    "    df_inc_jbs.groupby('industry')[['filled_jobs','sales']].cov().reset_index()\n",
    ")\n",
    "print(ind_std.sort_values(by='sales', ascending = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression plot by industry\n",
    "sns.lmplot(x='filled_jobs', y='sales', data=df_inc_jbs, col ='industry', aspect = 0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we see in the scatter plot that each industry's job filled and sales values are clearly defined from one another. <br>\n",
    "From our correlation results, we also see that almost all industries, with the exception of one, have positive correlation between actual jobs filled and sales. <br>\n",
    "Interestingly enough, Health Care and Social Assistance has the highest correlation of 0.98 (2 d.p.), which we can also visually see in the scatter plot. <br>\n",
    "Information Media and Telecommunications is the only industry with negative correlation (-0.35), as well as negative covariance. This may be because many parts of this sector are more automated and capital intensive, rather than labour intensive. Thus, sales does not increase linearly with filled jobs. This may also explain why Health Care and Social Assistance has the highest correlation value, as this job will be highly labour intensive. For example, the more doctors that are employed, the more patients can be treated, and therefore healthcare providers earn more treatment fees. <br>\n",
    "Another interesting point is that Construction has the highest sales covariance value. This makes sense, as the industry is highly labour intensive. Additionally, construction projects and income scales very closely with workforce size.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcenv_base36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
