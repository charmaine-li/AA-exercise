{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os       as os\n",
    "import numpy    as np\n",
    "import datetime as dt\n",
    "import sys      as sys\n",
    "\n",
    "##\n",
    "## Graphical libs and setup\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    " \n",
    "#print(plt.style.available)\n",
    "plt.style.use(['ggplot', 'fast'])\n",
    "plt.rc('image', cmap='cubehelix') # See https://www.mrao.cam.ac.uk/~dag/CUBEHELIX/\n",
    " \n",
    "import seaborn  as sns\n",
    "import pandas   as pd\n",
    "pd.options.display.width = 250\n",
    " \n",
    "from bnzds.utilities import CommonNotebook as cn\n",
    "from bnzds.utilities import CommonIO       as cio\n",
    "from bnzds.utilities import CommonGraphs   as cg\n",
    "from bnzds.utilities import CommonSpark    as cs\n",
    " \n",
    "# Not yet guaranteed to work under phttp://pxlbig03:10020/notebooks/DS_Python_Utilities/bnzds/examples/Jupyter/Lab_Test_BoxGraph.ipynb#ython2\n",
    "# ML libs should be on python3 anyway\n",
    "#if sys.version_info[0] >= 3:\n",
    "#    from bnzds.utilities import CommonML       as cml\n",
    "#    from bnzds.utilities import CommonTA       as cta\n",
    " \n",
    "## Set OUTPUT paths\n",
    "mainpath = os.environ.get('DSOUTPUT')\n",
    "if mainpath is None:\n",
    "    from os.path import expanduser\n",
    "    mainpath = expanduser(\"~\") + '/output/'\n",
    "\n",
    "dpath = mainpath + '/data/'\n",
    "gpath = mainpath + '/graphs/'\n",
    "mpath = mainpath + '/model/'\n",
    "                \n",
    "## Platform Init\n",
    "cn.platformInit(libs=[cn, cio], gpath=gpath)\n",
    "#print = cn.aprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con1 = duckdb.connect(':memory:', config={'allow_unsigned_extensions' : 'true'})\n",
    "\n",
    "# Edge node has 504G\n",
    "con1.sql(\"SET memory_limit = '100GB'\")  \n",
    "\n",
    "# See https://stackoverflow.com/questions/71952623/reading-partitioned-parquet-files-in-duckdb\n",
    "con1.sql(\"SET temp_directory = '/data/disk1/tmp/duckdbcaches/\" + os.environ.get('USER') + \"'\")\n",
    "\n",
    "# Use progress bar (if possible)\n",
    "print(con1.sql(\"PRAGMA enable_print_progress_bar\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin = pd.read_csv('business-financial-data-march-2024-csv.csv')\n",
    "df_fin.columns = [str(col).lower().replace(' ','_') for col in df_fin.columns]\n",
    "df_fin['year'] = df_fin['period'].astype(str).str.slice(0,4).astype(int)\n",
    "df_fin.rename(columns={'group':'ind_group'}, inplace='True')\n",
    "df_fin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp = pd.read_csv('machine-readable-business-employment-data-mar-2024-quarter.csv')\n",
    "df_emp.columns = [str(col).lower().replace(' ','_') for col in df_emp.columns]\n",
    "df_emp.rename(columns={'group':'ind_group'}, inplace='True')\n",
    "df_emp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.register('fin',df_fin)\n",
    "duckdb.register('emp',df_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_fin.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_query = \"\"\"\n",
    "with sal_wgs as\n",
    "(--Get all data where series_title_1 = salaries and wages\n",
    "select \n",
    "    *\n",
    "from fin \n",
    "\n",
    "where series_title_1 = 'Salaries and wages'\n",
    "      \n",
    ")\n",
    "\n",
    ", first_year as\n",
    "(--All industries where first year for salaries and wages after 2016\n",
    "select\n",
    "    series_title_2\n",
    "    , ind_group\n",
    "    , min(year) as first_year\n",
    "from sal_wgs\n",
    "\n",
    "group by series_title_2, ind_group\n",
    "\n",
    "having min(year) > 2016\n",
    ")\n",
    "\n",
    "select\n",
    "    series_title_2 as industry\n",
    "    ,avg_filled_jobs as overall_filled_jobs\n",
    "from \n",
    "(--Average actual filled jobs per industry\n",
    "    select\n",
    "        emp.series_title_2\n",
    "        ,avg(emp.data_value) as avg_filled_jobs\n",
    "        ,emp.series_title_1\n",
    "    from emp \n",
    "    \n",
    "    inner join first_year fst on fst.series_title_2 = emp.series_title_2 --Noted that emp dataset only includes NZSIOC Level 2\n",
    "    --Actual filled jobs only\n",
    "    where emp.series_title_1 = 'Filled jobs'\n",
    "          and emp.series_title_3 = 'Actual'\n",
    "\n",
    "    group by emp.series_title_2\n",
    "            , emp.series_title_1\n",
    ") mx\n",
    "\n",
    "order by avg_filled_jobs desc\n",
    "\n",
    "limit 1\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "result = duckdb.sql(Q1_query).df()\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2_query = \"\"\"\n",
    "with base as \n",
    "(--Get all data for business industry is level 2, and variables are seasonally adjusted operating income\n",
    "select \n",
    "    *\n",
    "from fin\n",
    "where series_title_1 = 'Sales (operating income)'\n",
    "      and ind_group = 'Industry by financial variable (NZSIOC Level 2)'\n",
    "      and series_title_4 = 'Seasonally adjusted'\n",
    ")\n",
    "\n",
    "\n",
    "select \n",
    "    period\n",
    "    ,series_title_2 as industry\n",
    "    ,data_value as operating_income\n",
    "from\n",
    "    (--Rank seasonally adjusted operating income largest to smallest across all periods and industries\n",
    "    select\n",
    "        period\n",
    "        ,series_title_2\n",
    "        ,data_value\n",
    "        ,dense_rank() over (order by data_value desc) as operating_inc_rnk\n",
    "    from base\n",
    "    ) rnk\n",
    "where operating_inc_rnk = 2\n",
    "\"\"\"\n",
    "\n",
    "result = duckdb.sql(Q2_query).df()\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3_query = \"\"\"\n",
    "with \n",
    "base as \n",
    "(--Get base data for territorial authorities and filled jobs\n",
    "select\n",
    "      series_title_2\n",
    "      ,data_value\n",
    "      ,period\n",
    "from emp\n",
    "where ind_group = 'Territorial authority by employment variable'\n",
    "      and series_title_1 = 'Filled jobs'\n",
    ")\n",
    "\n",
    ",highest_avg as\n",
    "(--Get the territorial authority with the highest average value for filled jobs \n",
    "select\n",
    "      series_title_2 as territorial_authority\n",
    "      ,avg_filled_jobs\n",
    "from      \n",
    "      (\n",
    "      select\n",
    "            series_title_2\n",
    "            ,avg(data_value) as avg_filled_jobs\n",
    "      from base\n",
    "      group by series_title_2\n",
    "      ) avrg\n",
    "order by avg_filled_jobs desc\n",
    "limit 1      \n",
    ")\n",
    "\n",
    ", sorted_ta as\n",
    "(\n",
    "select\n",
    "      base.period\n",
    "      ,ha.territorial_authority\n",
    "      ,base.data_value as filled_jobs\n",
    "from highest_avg as ha\n",
    "inner join base on base.series_title_2 = ha.territorial_authority\n",
    ")\n",
    "\n",
    "--Calculate quarterly cumulative number of filled jobs over time\n",
    "select      \n",
    "      sta.period\n",
    "      ,sta.filled_jobs\n",
    "      ,sum(prd.filled_jobs) as cumulative_filled_jobs\n",
    "from sorted_ta sta\n",
    "inner join sorted_ta prd on prd.period <= sta.period\n",
    "group by sta.period, sta.filled_jobs\n",
    "order by sta.period\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "result = duckdb.sql(Q3_query).df()\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Four\n",
    "\n",
    "We could use libraries like Pandas to check the data for such aberrations, and that they only include values we expect.\n",
    "\n",
    "For duplicates, we should first check if there are any duplicates, and drop them if so.\n",
    "/n\n",
    "e.g. df_fin[df_fin.duplicated() == True] to check for duplicates\n",
    "     df_fin.drop_duplicates to drop duplicates.\n",
    "/n\n",
    "To identify incorrect data types, we would first want to see what data types the datasets contain, and if they are what we expected, using df.dtypes.\n",
    "Since the datasets are part of a pipeline, we should know what columns and data types we expect to see for each.\n",
    "Thus, we can first define a dictionary showing the columns and their expected data types.\n",
    "For example, using the financial data csv, we may define something like the below, and then check the actual data types of our dataset against this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_schema = {\n",
    "    'series_reference': 'object',\n",
    "    'period': 'object',\n",
    "     'data_value': 'float64',\n",
    "     'suppressed': 'object',\n",
    "     'status': 'object',\n",
    "     'units': 'object',\n",
    "     'magnitude': 'int64',\n",
    "     'subject': 'object',\n",
    "     'ind_group': 'object',\n",
    "     'series_title_1': 'object',\n",
    "     'series_title_2': 'object',\n",
    "     'series_title_3': 'object',\n",
    "     'series_title_4': 'object',\n",
    "     'series_title_5': 'object',\n",
    "     'year': 'int64'\n",
    "}\n",
    "\n",
    "#Then, we would want to check the actual data types of our dataset against this dictionary using:\n",
    "\n",
    "for col, dtype in expected_schema.items():\n",
    "    assert df_fin[col].dtype == dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this, we will see an AssertionError because period and series_title_5 are not our expected data types of object.\n",
    "To improve the pipeline, a function could then be written to compare the expected data types against the actual, and if they are different, then cast the actual to the expected.\n",
    "e.g. The function could be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_schema(df_fin, expected_schema):\n",
    "    for col, expected_dtype in expected_schema.items():\n",
    "        actual_dtype = df_fin[col].dtype\n",
    "\n",
    "        # Skip if it's already correct\n",
    "        if str(actual_dtype) == expected_dtype:\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Column '{col}' is {actual_dtype}, expected {expected_dtype}. Attempting to cast...\")\n",
    "\n",
    "        try:\n",
    "            if 'float' in expected_dtype:\n",
    "                df_fin[col] = pd.to_numeric(df_fin[col], errors='coerce')\n",
    "            elif 'int' in expected_dtype:\n",
    "                df_fin[col] = pd.to_numeric(df_fin[col], errors='coerce').astype('Int64')  # nullable int\n",
    "            elif expected_dtype == 'object':\n",
    "                df_fin[col] = df_fin[col].astype(str)\n",
    "            elif 'datetime' in expected_dtype:\n",
    "                df_fin[col] = pd.to_datetime(df_fin[col], errors='coerce')\n",
    "            else:\n",
    "                print(f\"[WARN] Unhandled type: {expected_dtype}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to cast column '{col}' to {expected_dtype}: {e}\")\n",
    "\n",
    "    return df_fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_stg = enforce_schema(df_fin,expected_schema)\n",
    "print(df_fin.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcenv_base36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
