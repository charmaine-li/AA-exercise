{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os       as os\n",
    "import numpy    as np\n",
    "import datetime as dt\n",
    "import sys      as sys\n",
    "\n",
    "##\n",
    "## Graphical libs and setup\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    " \n",
    "#print(plt.style.available)\n",
    "plt.style.use(['ggplot', 'fast'])\n",
    "plt.rc('image', cmap='cubehelix') # See https://www.mrao.cam.ac.uk/~dag/CUBEHELIX/\n",
    " \n",
    "import seaborn  as sns\n",
    "import pandas   as pd\n",
    "pd.options.display.width = 250\n",
    " \n",
    "from bnzds.utilities import CommonNotebook as cn\n",
    "from bnzds.utilities import CommonIO       as cio\n",
    "from bnzds.utilities import CommonGraphs   as cg\n",
    "from bnzds.utilities import CommonSpark    as cs\n",
    " \n",
    "# Not yet guaranteed to work under phttp://pxlbig03:10020/notebooks/DS_Python_Utilities/bnzds/examples/Jupyter/Lab_Test_BoxGraph.ipynb#ython2\n",
    "# ML libs should be on python3 anyway\n",
    "#if sys.version_info[0] >= 3:\n",
    "#    from bnzds.utilities import CommonML       as cml\n",
    "#    from bnzds.utilities import CommonTA       as cta\n",
    " \n",
    "## Set OUTPUT paths\n",
    "mainpath = os.environ.get('DSOUTPUT')\n",
    "if mainpath is None:\n",
    "    from os.path import expanduser\n",
    "    mainpath = expanduser(\"~\") + '/output/'\n",
    "\n",
    "dpath = mainpath + '/data/'\n",
    "gpath = mainpath + '/graphs/'\n",
    "mpath = mainpath + '/model/'\n",
    "                \n",
    "## Platform Init\n",
    "cn.platformInit(libs=[cn, cio], gpath=gpath)\n",
    "#print = cn.aprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con1 = duckdb.connect(':memory:', config={'allow_unsigned_extensions' : 'true'})\n",
    "\n",
    "# Edge node has 504G\n",
    "con1.sql(\"SET memory_limit = '100GB'\")  \n",
    "\n",
    "# See https://stackoverflow.com/questions/71952623/reading-partitioned-parquet-files-in-duckdb\n",
    "con1.sql(\"SET temp_directory = '/data/disk1/tmp/duckdbcaches/\" + os.environ.get('USER') + \"'\")\n",
    "\n",
    "# Use progress bar (if possible)\n",
    "print(con1.sql(\"PRAGMA enable_print_progress_bar\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin = pd.read_csv('business-financial-data-march-2024-csv.csv')\n",
    "df_fin.columns = [str(col).lower().replace(' ','_') for col in df_fin.columns]\n",
    "df_fin['year'] = df_fin['period'].astype(str).str.slice(0,4).astype(int)\n",
    "df_fin.rename(columns={'group':'ind_group'}, inplace='True')\n",
    "df_fin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp = pd.read_csv('machine-readable-business-employment-data-mar-2024-quarter.csv')\n",
    "df_emp.columns = [str(col).lower().replace(' ','_') for col in df_emp.columns]\n",
    "df_emp.rename(columns={'group':'ind_group'}, inplace='True')\n",
    "df_emp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.register('fin',df_fin)\n",
    "duckdb.register('emp',df_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_fin.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_query = \"\"\"\n",
    "with sal_wgs as\n",
    "(--Get all data where series_title_1 = salaries and wages\n",
    "select \n",
    "    *\n",
    "from fin \n",
    "\n",
    "where series_title_1 = 'Salaries and wages'\n",
    "      \n",
    ")\n",
    "\n",
    ", first_year as\n",
    "(--All industries where first year for salaries and wages after 2016\n",
    "select\n",
    "    series_title_2\n",
    "    , ind_group\n",
    "    , min(year) as first_year\n",
    "from sal_wgs\n",
    "\n",
    "group by series_title_2, ind_group\n",
    "\n",
    "having min(year) > 2016\n",
    ")\n",
    "\n",
    "select\n",
    "    series_title_2 as industry\n",
    "    ,avg_filled_jobs as overall_filled_jobs\n",
    "from \n",
    "(--Average actual filled jobs per industry\n",
    "    select\n",
    "        emp.series_title_2\n",
    "        ,avg(emp.data_value) as avg_filled_jobs\n",
    "        ,emp.series_title_1\n",
    "    from emp \n",
    "    \n",
    "    inner join first_year fst on fst.series_title_2 = emp.series_title_2 --Noted that emp dataset only includes NZSIOC Level 2\n",
    "    --Actual filled jobs only\n",
    "    where emp.series_title_1 = 'Filled jobs'\n",
    "          and emp.series_title_3 = 'Actual'\n",
    "\n",
    "    group by emp.series_title_2\n",
    "            , emp.series_title_1\n",
    ") mx\n",
    "\n",
    "order by avg_filled_jobs desc\n",
    "\n",
    "limit 1\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "result = duckdb.sql(Q1_query).df()\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2_query = \"\"\"\n",
    "with base as \n",
    "(--Get all data for business industry is level 2, and variables are seasonally adjusted operating income\n",
    "select \n",
    "    *\n",
    "from fin\n",
    "where series_title_1 = 'Sales (operating income)'\n",
    "      and ind_group = 'Industry by financial variable (NZSIOC Level 2)'\n",
    "      and series_title_4 = 'Seasonally adjusted'\n",
    ")\n",
    "\n",
    "\n",
    "select \n",
    "    period\n",
    "    ,series_title_2 as industry\n",
    "    ,data_value as operating_income\n",
    "from\n",
    "    (--Rank seasonally adjusted operating income largest to smallest across all periods and industries\n",
    "    select\n",
    "        period\n",
    "        ,series_title_2\n",
    "        ,data_value\n",
    "        ,dense_rank() over (order by data_value desc) as operating_inc_rnk\n",
    "    from base\n",
    "    ) rnk\n",
    "where operating_inc_rnk = 2\n",
    "\"\"\"\n",
    "\n",
    "result = duckdb.sql(Q2_query).df()\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3_query = \"\"\"\n",
    "with \n",
    "base as \n",
    "(--Get base data for territorial authorities and filled jobs\n",
    "select\n",
    "      series_title_2\n",
    "      ,data_value\n",
    "      ,period\n",
    "from emp\n",
    "where ind_group = 'Territorial authority by employment variable'\n",
    "      and series_title_1 = 'Filled jobs'\n",
    ")\n",
    "\n",
    ",highest_avg as\n",
    "(--Get the territorial authority with the highest average value for filled jobs \n",
    "select\n",
    "      series_title_2 as territorial_authority\n",
    "      ,avg_filled_jobs\n",
    "from      \n",
    "      (\n",
    "      select\n",
    "            series_title_2\n",
    "            ,avg(data_value) as avg_filled_jobs\n",
    "      from base\n",
    "      group by series_title_2\n",
    "      ) avrg\n",
    "order by avg_filled_jobs desc\n",
    "limit 1      \n",
    ")\n",
    "\n",
    ", sorted_ta as\n",
    "(\n",
    "select\n",
    "      base.period\n",
    "      ,ha.territorial_authority\n",
    "      ,base.data_value as filled_jobs\n",
    "from highest_avg as ha\n",
    "inner join base on base.series_title_2 = ha.territorial_authority\n",
    ")\n",
    "\n",
    "--Calculate quarterly cumulative number of filled jobs over time\n",
    "select      \n",
    "      sta.period\n",
    "      ,sta.filled_jobs\n",
    "      ,sum(prd.filled_jobs) as cumulative_filled_jobs\n",
    "from sorted_ta sta\n",
    "inner join sorted_ta prd on prd.period <= sta.period\n",
    "group by sta.period, sta.filled_jobs\n",
    "order by sta.period\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "result = duckdb.sql(Q3_query).df()\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Four\n",
    "\n",
    "We could use libraries like Pandas to check the data for such aberrations, and that they only include values we expect. <br>\n",
    "\n",
    "For duplicates, we should first check if there are any duplicates, and drop them if so. <br>\n",
    "\n",
    "e.g. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_dedupe = df_fin\n",
    "df_fin_dedupe[df_fin_dedupe.duplicated() == True] # to check for duplicates <br>\n",
    "df_fin_dedupe.drop_duplicates # to drop duplicates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify incorrect data types, we would first want to see what data types the datasets contain, and if they are what we expected, using df.dtypes.\n",
    "Since the datasets are part of a pipeline, we should know what columns and data types we expect to see for each. <br>\n",
    "Thus, we can first define a dictionary showing the columns and their expected data types.\n",
    "For example, using the financial data csv, we may define something like the below, and then check the actual data types of our dataset against this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_schema = {\n",
    "    'series_reference': 'object',\n",
    "    'period': 'object',\n",
    "     'data_value': 'float64',\n",
    "     'suppressed': 'object',\n",
    "     'status': 'object',\n",
    "     'units': 'object',\n",
    "     'magnitude': 'int64',\n",
    "     'subject': 'object',\n",
    "     'ind_group': 'object',\n",
    "     'series_title_1': 'object',\n",
    "     'series_title_2': 'object',\n",
    "     'series_title_3': 'object',\n",
    "     'series_title_4': 'object',\n",
    "     'series_title_5': 'object',\n",
    "     'year': 'int64'\n",
    "}\n",
    "\n",
    "#Then, we would want to check the actual data types of our dataset against this dictionary using:\n",
    "\n",
    "for col, dtype in expected_schema.items():\n",
    "    assert df_fin_dedupe[col].dtype == dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this, we will see an AssertionError because period and series_title_5 are not our expected data types of object. <br>\n",
    "To improve the pipeline, a function could then be written to compare the expected data types against the actual, and if they are different, then cast the actual to the expected. <br>\n",
    "e.g. The function could be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_schema(df_fin_dedupe, expected_schema):\n",
    "    for col, expected_dtype in expected_schema.items():\n",
    "        actual_dtype = df_fin[col].dtype\n",
    "\n",
    "        # Skip if it's already correct\n",
    "        if str(actual_dtype) == expected_dtype:\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Column '{col}' is {actual_dtype}, expected {expected_dtype}. Attempting to cast...\")\n",
    "\n",
    "        try:\n",
    "            if 'float' in expected_dtype:\n",
    "                df_fin[col] = pd.to_numeric(df_fin[col], errors='coerce')\n",
    "            elif 'int' in expected_dtype:\n",
    "                df_fin[col] = pd.to_numeric(df_fin[col], errors='coerce').astype('Int64')  # nullable int\n",
    "            elif expected_dtype == 'object':\n",
    "                df_fin[col] = df_fin[col].fillna('').astype(str)\n",
    "            elif 'datetime' in expected_dtype:\n",
    "                df_fin[col] = pd.to_datetime(df_fin[col], errors='coerce')\n",
    "            else:\n",
    "                print(f\"[WARN] Unhandled type: {expected_dtype}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to cast column '{col}' to {expected_dtype}: {e}\")\n",
    "\n",
    "    return df_fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_stg = enforce_schema(df_fin_dedupe,expected_schema)\n",
    "print(df_fin_stg.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for missing dates, we may want to define the range of dates we are expecting for each dataset.\n",
    "From there, we can check if the dataset is missing any of these dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the date range we are expecting\n",
    "expected_periods = pd.date_range('2016-01-01','2024-03-31', freq = 'Q')\n",
    "#Define missing dates as ones not within the expected_periods\n",
    "missing_periods = set(expected_periods) - set(pd.to_datetime(df_fin['period'], format = '%Y.%m')+ pd.offsets.MonthEnd(0))\n",
    "\n",
    "missing_periods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For aberrations such as other missing values, we may want to drop any rows with missing data, or fill with another value, such 'Unknown'. This would help prevent misinterpretation of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop any rows with NULL in data_value column\n",
    "df_fin_na = df_fin_stg.dropna(subset = ['data_value'])\n",
    "\n",
    "#Fill any NAs or missing values for suppressed and series_title_5 with 'Unknown'\n",
    "df_fin_fill = df_fin_stg.replace('',pd.NA).fillna({\n",
    "    'suppressed': 'Unknown',\n",
    "    'series_title_5': 'Unknown'\n",
    "})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may also be worth considering why certain values are missing to assist with analysis further down the line.\n",
    "For example, are they missing completely at random, missing at random or missing not at random? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Five\n",
    "We will explore if there is a correlation between actual filled jobs and unadjusted sales(operating income) for the NZSIOC Level 2 industries per period. <br>\n",
    "We should expect to see a positive correlation between the two, as generally more labour will result in higher productivity, and therefore higher sales. <br>\n",
    "However, we should expect this to vary significantly between industries, as well as see clearly defined cut-offs between industries in number of filled jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter employment data for relevant observations\n",
    "df_emp_jbs = df_emp[\n",
    "    (df_emp['ind_group']=='Industry by employment variable') &\n",
    "    (df_emp['series_title_1']=='Filled jobs') &\n",
    "    (df_emp['series_title_3']=='Actual')\n",
    "    ]\n",
    "\n",
    "# Filter financial data for relevant observations\n",
    "df_fin_inc = df_fin[\n",
    "    (df_fin['ind_group']== 'Industry by financial variable (NZSIOC Level 2)') &\n",
    "    (df_fin['series_title_1']=='Sales (operating income)') &\n",
    "    (df_fin['series_title_4']=='Unadjusted')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_inc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp_jbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_summary = df_emp_jbs.groupby(['period','series_title_2']).agg({'data_value':'mean'}).reset_index()\n",
    "emp_summary.rename(columns={'data_value':'filled_jobs', 'series_title_2': 'industry'}, inplace = True)\n",
    "emp_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_summary = df_fin_inc.groupby(['period','series_title_2']).agg({'data_value':'mean'}).reset_index()\n",
    "inc_summary.rename(columns={'data_value':'sales', 'series_title_2': 'industry'}, inplace = True)\n",
    "inc_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inc_jbs = pd.merge(emp_summary, inc_summary, on =['period','industry'])\n",
    "df_inc_jbs.dropna(inplace = True)\n",
    "df_inc_jbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "df_inc_jbs.describe()\n",
    "df_inc_jbs['industry'].value_counts()\n",
    "df_inc_jbs['sales'].groupby(df_inc_jbs['industry']).mean()\n",
    "df_inc_jbs['filled_jobs'].groupby(df_inc_jbs['industry']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(data=df_inc_jbs, x='filled_jobs', y='sales', hue ='industry', style ='industry', palette = 'viridis')\n",
    "plt.title('Filled Jobs vs Sales (Operating Income) by Industry')\n",
    "plt.xlabel('Filled Jobs')\n",
    "plt.ylabel('Sales (Operating Income)')\n",
    "plt.legend(bbox_to_anchor=(1.05,1), loc = 'upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of filled jobs to sales by industry\n",
    "ind_corrs = (\n",
    "    df_inc_jbs.groupby('industry')[['filled_jobs','sales']].corr().unstack().iloc[:,1].reset_index(name ='correlation')\n",
    ")\n",
    "print(ind_corrs.sort_values(by='correlation',ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance of filled jobs and sales by industry\n",
    "ind_std = (\n",
    "    df_inc_jbs.groupby('industry')[['filled_jobs','sales']].cov().reset_index()\n",
    ")\n",
    "print(ind_std.sort_values(by='sales', ascending = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression plot by industry\n",
    "sns.lmplot(x='filled_jobs', y='sales', data=df_inc_jbs, col ='industry', aspect = 0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we see in the scatter plot that each industry's job filled and sales values are clearly defined from one another. <br>\n",
    "From our correlation results, we also see that almost all industries, with the exception of one, have positive correlation between actual jobs filled and sales. <br>\n",
    "Interestingly enough, Health Care and Social Assistance has the highest correlation of 0.98 (2 d.p.), which we can also visually see in the scatter plot. <br>\n",
    "Information Media and Telecommunications is the only industry with negative correlation (-0.35), as well as negative covariance. This may be because many parts of this sector are more automated and capital intensive, rather than labour intensive. Thus, sales does not increase linearly with filled jobs. This may also explain why Health Care and Social Assistance has the highest correlation value, as this job will be highly labour intensive. For example, the more doctors that are employed, the more patients can be treated, and therefore healthcare providers earn more treatment fees. <br>\n",
    "Another interesting point is that Construction has the highest sales covariance value. This makes sense, as the industry is highly labour intensive. Additionally, construction projects and income scales very closely with workforce size.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcenv_base36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
